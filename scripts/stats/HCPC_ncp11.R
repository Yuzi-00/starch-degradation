
library(tidyverse)

library(FactoMineR)

library(factoextra)

df <- read_csv("analysis/total_new_convert.csv")

# select the variables

df <- df %>%
  filter(Sample != "C+" & Sample != "C-") %>%
  na.omit()

# calculate the mean value of the kinetics 

df1 <- df %>%
  select(2, 29:31) %>%
  group_by(Sample) %>%
  summarise(k = mean(k), h = mean(h), Xinf = mean(Xinf)) 

# select the structural properties

df2 <- df %>%
  select(2, 5:19) %>% 
  unique()

# combine and scale

df_new <- full_join(df2, df1) %>%
  select(-Sample)

# compute PCA

res.pca <- PCA(df_new, 
               ncp = 11, 
               scale.unit = TRUE, 
               quanti.sup = 16:18, 
               graph = FALSE)

summary(res.pca)

#### HCPC ####

# Compute hierarchical clustering on principal components

res.hcpc <- HCPC(res.pca, graph = FALSE,
                 consol = TRUE)

# visualize the dendrogram generated by the hierarchical clustering

fviz_dend(res.hcpc, 
          cex = 0.7, # label size
          palette = "jco",
          rect = TRUE, rect_fill = TRUE,
          rect_border = "jco",         
          labels_track_height = 0.8)

## the dentrogram suggest 3 clusters solution

# visualize individuals on the principal component map 
# and to color individuals according to the cluster they belong to (factorial map)

fviz_cluster(res.hcpc,
             repel = FALSE, # avoid label overlapping
             show.clust.cent = TRUE, # show cluster centers
             palette = "jco", 
             ggtheme = theme_minimal(),
             main = "Factor map"
)

# we can also draw a three dimensional plot combining the hierarchical 
# clustering and the factorial map (principal components + tree)

# plot(res.hcpc, choice = "3D.map") # well, figure margins too large. FAILED

# the function HCPC() returns a list containing:
##   data.clust: The original data with a supplementary 
##   column called class containing the partition.
##   desc.var: The variables describing clusters
##   desc.ind: The more typical individuals of each cluster
##   desc.axes: The axes describing clusters

# display the original data with cluster assignments

head(res.hcpc$data.clust, 10)

## the last column contains the cluster assignments

# display quantitative variables that describe the most of each cluster

res.hcpc$desc.var$quanti

# show principal dimensions that are the most associated with clusters

res.hcpc$desc.axes$quanti

# representative individuals of each cluster can be extracted as follow

res.hcpc$desc.ind$para

## for each cluster, the top 5 closest individuals to the cluster center is shown

# extract the samples that belong to each cluster 

c1 <- res.hcpc$data.clust %>%
  filter(clust == 1)

write_csv(c1, "analysis/cluster1.csv")

c2 <- res.hcpc$data.clust %>%
  filter(clust == 2)

write_csv(c2, "analysis/cluster2.csv")

c3 <- res.hcpc$data.clust %>%
  filter(clust == 3)

write_csv(c3, "analysis/cluster3.csv")

## these subclasses will be used to do the corrplot in the same way as before

#### k-means clustering ####

kc <- kmeans(res.pca$svd$U, 3)

kc

plot(res.pca$svd$U[,1:2],col=factor(kc$cluster))

fviz_cluster(kc, data = res.pca$svd$U)


